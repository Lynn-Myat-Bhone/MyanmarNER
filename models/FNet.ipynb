{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "d18b8086-92ad-40e0-b2c0-ed7ce97b36b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N-_RXQTAqWhl",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ebc74ea7",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ebc74ea7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,filepath):\n",
        "        self.sentences , self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self,filepath):\n",
        "        sentences, ner_tags = [],[]\n",
        "        sentence, ner_tag = [],[]\n",
        "        with open(filepath,\"r\",encoding = \"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    word,_,ner = line.split(\"\\t\")\n",
        "                    sentence.append(word)\n",
        "                    ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "            return sentences,ner_tags\n",
        "    def __len__(self):\n",
        "      return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "      return self.sentences[idx], self.ner_tags[idx]\n",
        "\n",
        "def collate_fn(self,batch):\n",
        "  sentences,tag = zip(*batch)\n",
        "  max_len = max(len(s) for s in sentences)\n",
        "  padded_sentences = [  s+ [0] * (max_len-len(s)) for s in sentences]\n",
        "  padded_tag = [ t + [0] *(max_len-len(t)) for t in tag]\n",
        "\n",
        "  return torch.tensor(padded_sentences),torch.tensor(padded_tag)"
      ],
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "id": "1DUeTjUbqMXk",
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "id": "VP9MDl5CshWD"
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\"\n",
        "\n",
        "train_data = CustomDataset(train_path)\n",
        "val_data = CustomDataset(val_path)\n",
        "test_data = CustomDataset(test_path)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "LVhMJrOhqTbL"
      },
      "id": "LVhMJrOhqTbL",
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}