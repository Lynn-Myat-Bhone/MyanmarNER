{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 141,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "02c34e94-1ceb-49e0-fafa-6838fa054c7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "cYFjp3ekLUf1"
      },
      "id": "cYFjp3ekLUf1",
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath):\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # for both word and tag\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "    attention_masks = [[1]*len(s) + [0]*(max_len - len(s)) for s in sentences]\n",
        "\n",
        "    return (\n",
        "        torch.tensor(padded_sentences, dtype=torch.long),\n",
        "        torch.tensor(padded_tags, dtype=torch.long),\n",
        "        torch.tensor(attention_masks, dtype=torch.bool),\n",
        "    )\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\""
      ],
      "metadata": {
        "id": "uhkHXmoz4Lhp"
      },
      "id": "uhkHXmoz4Lhp",
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Force reinstall compatible versions\n",
        "# !pip install gensim\n",
        "# !pip install numpy==1.24.3 --force-reinstall\n",
        "# !pip install pytorch-crf"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0"
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": 172,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = CustomDataset(train_path)\n",
        "val_data = CustomDataset(val_path)\n",
        "test_data = CustomDataset(test_path)"
      ],
      "metadata": {
        "id": "yKsmjTRK-GAk"
      },
      "id": "yKsmjTRK-GAk",
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create vocabulary and tag-to-index mappings\n",
        "vocab = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "ner_tag_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}  # Start with <PAD> and <UNK>\n",
        "\n",
        "# Build vocab and tag mappings\n",
        "for dataset in [train_data, val_data, test_data]:\n",
        "    for sentence, ner_tags in zip(dataset.sentences, dataset.ner_tags):\n",
        "        for word in sentence:\n",
        "            if word not in vocab:\n",
        "                vocab[word] = len(vocab)\n",
        "        for ner_tag in ner_tags:\n",
        "            if ner_tag not in ner_tag_to_ix:\n",
        "                ner_tag_to_ix[ner_tag] = len(ner_tag_to_ix)\n",
        "\n",
        "embedding_dim = 300  # FastText embedding dimension\n",
        "embedding_matrix = []\n",
        "for word in vocab:\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    elif word == \"<PAD>\":\n",
        "        embedding_matrix.append(np.zeros(embedding_dim))\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.normal(scale=0.6, size=(embedding_dim,)))\n",
        "\n",
        "fasttext_embeddings = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "ix_to_ner_tag = {v: k for k, v in ner_tag_to_ix.items()}"
      ],
      "metadata": {
        "id": "qtqQEtlC9-A-"
      },
      "id": "qtqQEtlC9-A-",
      "execution_count": 208,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in [train_data, val_data, test_data]:\n",
        "    dataset.word2idx = vocab\n",
        "    dataset.tag2idx = ner_tag_to_ix\n"
      ],
      "metadata": {
        "id": "pQKWfwLfPTtt"
      },
      "id": "pQKWfwLfPTtt",
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 256\n",
        "vocab_size = len(vocab)\n",
        "num_ner_tags = len(ner_tag_to_ix)\n",
        "print(vocab_size)\n",
        "print(num_ner_tags)"
      ],
      "metadata": {
        "id": "tWW8WiRU_s6J",
        "outputId": "23cec5b5-0203-4cf8-c6f4-ae300a19c4ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tWW8WiRU_s6J",
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "19304\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from torchcrf import CRF\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, d_model):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch_size, seq_len, d_model)\n",
        "        x = x + self.pe[:x.size(1)].unsqueeze(0)  # Fix: broadcast shape correctly\n",
        "        return x\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))\n",
        "        return self.dropout2(self.fc2(x))\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)\n",
        "        x = self.act(x.real)\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNetBlock(nn.Module):\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FNetBlock, self).__init__()\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        return self.norm2(x + residual)\n",
        "\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3, num_layers=2):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.pos_en = PositionalEncoding(256, d_model=dim)\n",
        "\n",
        "        # Stack multiple FNet blocks\n",
        "        self.encoder = nn.Sequential(*[\n",
        "            FNetBlock(dim, expension_factor, dropout) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "        self.crf = CRF(num_tags, batch_first=True)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        x = self.embedding(x)                  # [batch_size, seq_len, dim]\n",
        "        x = self.embedding_dropout(x)\n",
        "        x = self.pos_en(x)\n",
        "        x = self.encoder(x)                    # Stacked FNet blocks\n",
        "        emissions = self.classifier(x)         # [batch_size, seq_len, num_tags]\n",
        "        return emissions\n",
        "\n",
        "    def loss(self, emissions, tags, mask):\n",
        "        return -self.crf(emissions, tags, mask=mask, reduction='mean')\n",
        "\n",
        "    def decode(self, emissions, mask):\n",
        "        return self.crf.decode(emissions, mask=mask)"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": 211,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.utils.class_weight import compute_class_weight\n",
        "# labels = []\n",
        "# for tag_seq in train_data.ner_tags:\n",
        "#     labels.extend(tag_seq)\n",
        "\n",
        "\n",
        "# label_ids = [tag2idx[tag] for tag in labels]\n",
        "# present_classes = np.unique(label_ids)\n",
        "# present_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=label_ids)\n",
        "\n",
        "# full_weights = np.ones(len(tag2idx))\n",
        "# for i, cls in enumerate(present_classes):\n",
        "#     full_weights[cls] = present_weights[i]\n",
        "\n",
        "# weights = torch.tensor(full_weights, dtype=torch.float).to(device)"
      ],
      "metadata": {
        "id": "DJN2lrE_uKAV"
      },
      "id": "DJN2lrE_uKAV",
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FNetNER(embedding_matrix=fasttext_embeddings, num_tags=len(ner_tag_to_ix)).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4,weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train process\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for sentences, tags, attention_mask in train_data:\n",
        "        sentences = sentences.to(device)\n",
        "        tags = tags.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        attention_mask = attention_mask.bool()\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        emissions = model(sentences)  # [B, L, num_tags]\n",
        "        loss = model.loss(emissions, tags, attention_mask)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      for sentences,tags,attention_mask in val_data:\n",
        "          sentences = sentences.to(device)\n",
        "          tags = tags.to(device)\n",
        "          attention_mask = attention_mask.to(device)\n",
        "          attention_mask = attention_mask.bool()\n",
        "\n",
        "          emissions = model(sentences)\n",
        "          loss = model.loss(emissions,tags,attention_mask)\n",
        "          val_loss += loss.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_data)\n",
        "    avg_train_loss = total_loss / len(train_data)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 369
        },
        "outputId": "aa660ad0-bba9-4c6e-ff45-eb90fa74978d"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'CustomDataset' object has no attribute 'word2idx'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-213-501e8b6fcb34>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-450f12787431>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mword_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<UNK>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtag_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<UNK>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-205-450f12787431>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mword_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<UNK>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mtag_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"<UNK>\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mner_tags\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mword_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'CustomDataset' object has no attribute 'word2idx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's parameters\n",
        "torch.save(model.state_dict(), \"ner_model.pth\")"
      ],
      "metadata": {
        "id": "zekIuzU_BSry"
      },
      "id": "zekIuzU_BSry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_crf(model, dataloader, device, idx2tag):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags, attention_mask in dataloader:\n",
        "            sentences = sentences.to(device)\n",
        "            tags = tags.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "\n",
        "            emissions = model(sentences)                      # [batch_size, seq_len, num_tags]\n",
        "            mask = attention_mask.bool()\n",
        "            pred_seq = model.decode(emissions, mask)         # List[List[int]]\n",
        "\n",
        "            for i in range(len(tags)):\n",
        "                seq_len = mask[i].sum().item()\n",
        "                pred_tags = [idx2tag[idx] for idx in pred_seq[i][:seq_len]]\n",
        "                true_tags = [idx2tag[idx.item()] for idx in tags[i][:seq_len]]\n",
        "                all_preds.extend(pred_tags)\n",
        "                all_labels.extend(true_tags)\n",
        "\n",
        "    return all_preds, all_labels"
      ],
      "metadata": {
        "id": "tbyIKjRmTC2W"
      },
      "id": "tbyIKjRmTC2W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "test_preds, test_labels = evaluate_crf(model, test_load, device, idx2tag)\n",
        "print(classification_report(test_labels, test_preds))\n"
      ],
      "metadata": {
        "id": "IWQV_p2_1I-3"
      },
      "id": "IWQV_p2_1I-3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test case\n",
        "test_sentence = [\"ကြက်\", \"ခြေ\", \"နီ\", \"မှ\", \"ပြော\", \"ရေး\", \"ဆို\", \"ခွင့်\", \"ရှိ\", \"သူ\", \"MattCochrane\", \"သည်။\"]\n",
        "\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "mask = torch.ones_like(input_tensor, dtype=torch.bool).to(device)\n",
        "\n",
        "# Predict using CRF\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    emissions = model(input_tensor)  # returns emissions\n",
        "    best_paths = model.decode(emissions, mask)  # List of list: [ [tag_id, tag_id, ...] ]\n",
        "    predicted_ids = best_paths[0]  # since batch size is 1\n",
        "\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "\n",
        "# Print results\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "l2xguixJ6AP3"
      },
      "id": "l2xguixJ6AP3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "WtsFBuwgcU9r"
      },
      "id": "WtsFBuwgcU9r"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}