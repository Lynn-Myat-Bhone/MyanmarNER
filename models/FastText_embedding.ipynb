{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV2Z1H2VAkCg",
        "outputId": "20d0c1b1-74c4-416c-9bf6-f8ffa4a611ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "W9p3zDCwBwcB"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ydsh8p7pCYRg"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,filepath,word2idx,tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences , self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self,filepath):\n",
        "        sentences, ner_tags = [],[]\n",
        "        sentence, ner_tag = [],[]\n",
        "        with open(filepath,\"r\",encoding = \"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    word,_,ner = line.split(\"\\t\")\n",
        "                    sentence.append(word)\n",
        "                    ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []  # rest after every sentence\n",
        "            return sentences,ner_tags\n",
        "    def __len__(self):\n",
        "      return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"\"]) for t in self.ner_tags[idx]]\n",
        "\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "  sentences,tag = zip(*batch)\n",
        "  max_len = max(len(s) for s in sentences)\n",
        "  padded_sentences = [  s+ [0] * (max_len-len(s)) for s in sentences]\n",
        "  padded_tag = [ t + [0] *(max_len-len(t)) for t in tag]\n",
        "\n",
        "  return torch.tensor(padded_sentences),torch.tensor(padded_tag)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BXFI_HwCcdH"
      },
      "source": [
        "##### Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "njEAquwwCaB-"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/MyanmarNER/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/MyanmarNER/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/MyanmarNER/test_v5.conll\"\n",
        "\n",
        "word2idx = {\"\": 0, \"\": 1}\n",
        "tag2idx = {\"\": 0, \"\": 1}\n",
        "train_data = CustomDataset(train_path,word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path,word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path,word2idx, tag2idx)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n",
        "test_load = DataLoader(test_data, batch_size=32, collate_fn=collate_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYpoBibnCsQA",
        "outputId": "4e1d70a2-00f6-43b1-fd48-305bc84957de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences shape: torch.Size([32, 41])\n",
            "Tags shape: torch.Size([32, 41])\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Sentences shape:\", sentences.shape)\n",
        "    print(\"Tags shape:\", tags.shape)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMHW1nxhOaOo"
      },
      "source": [
        "##### Pretrained FastText Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "collapsed": true,
        "id": "teFpHZOUOZZl",
        "outputId": "9a704c11-f0c5-42f1-9e17-93ee3d71587b"
      },
      "outputs": [],
      "source": [
        "# Force reinstall compatible versions\n",
        "# !pip install numpy==1.24.3 --force-reinstall\n",
        "# !pip install gensim --force-reinstall\n",
        "\n",
        "%pip install numpy==1.24.3 --force-reinstall\n",
        "%pip install gensim --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2aPEkfrTPh7"
      },
      "outputs": [],
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load FastText Burmese .vec file (ensure it's already uploaded or on your drive)\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/MyanmarNER/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xBLy3u9FTi4_"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "\n",
        "def build_vocab(dataset):\n",
        "    word_set = set()\n",
        "    for sentence in dataset.sentences:\n",
        "        for word in sentence:\n",
        "            word_set.add(word)\n",
        "    return word_set\n",
        "\n",
        "# Create vocab from dataset (not DataLoader!)\n",
        "vocab = build_vocab(train_data)\n",
        "\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "\n",
        "# Initialize PAD and UNK\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # <PAD>\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # <UNK>\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = np.array(embedding_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XuW6AO90UQwt"
      },
      "outputs": [],
      "source": [
        "train_data = CustomDataset(\"/content/drive/MyDrive/MyanmarNER/train_v5.conll\", word2idx, tag2idx)\n",
        "val_data = CustomDataset(\"/content/drive/MyDrive/MyanmarNER/val_v5.conll\", word2idx, tag2idx)\n",
        "test_data = CustomDataset(\"/content/drive/MyDrive/MyanmarNER/test_v5.conll\", word2idx, tag2idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "LHxqHZIyUeGd"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class NERModel(nn.Module):\n",
        "    def __init__(self, embedding_matrix, hidden_dim, tagset_size):\n",
        "        super(NERModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.FloatTensor(embedding_matrix), freeze=False\n",
        "        )\n",
        "        self.lstm = nn.LSTM(input_size=embedding_matrix.shape[1],\n",
        "                            hidden_size=hidden_dim,\n",
        "                            num_layers=1,\n",
        "                            bidirectional=True,\n",
        "                            batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, tagset_size)  # because it's bidirectional\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embeds = self.embedding(input_ids)\n",
        "        lstm_out, _ = self.lstm(embeds)\n",
        "        output = self.fc(lstm_out)\n",
        "        return output\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "DCfbmaT3Uy1f"
      },
      "outputs": [],
      "source": [
        "model = NERModel(embedding_matrix=embedding_matrix, hidden_dim=128, tagset_size=len(tag2idx))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
