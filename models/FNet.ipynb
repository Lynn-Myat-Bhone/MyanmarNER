{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "05a34e7e-74c3-430e-fb39-53a4b3cdb607",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath, word2idx, tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # word2idx[\"<PAD>\"]\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "\n",
        "    return torch.tensor(padded_sentences, dtype=torch.long), torch.tensor(padded_tags, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_tags(filepath):\n",
        "    tag_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    tag = parts[2]\n",
        "                    tag_set.add(tag)\n",
        "    return sorted(tag_set)\n"
      ],
      "metadata": {
        "id": "3-4hXt0iT3eL"
      },
      "id": "3-4hXt0iT3eL",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "LVhMJrOhqTbL",
      "metadata": {
        "id": "LVhMJrOhqTbL"
      },
      "outputs": [],
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\"\n",
        "\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "train_data = CustomDataset(train_path,word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path,word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path,word2idx, tag2idx)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_tags = extract_unique_tags(train_path)\n",
        "tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "tag2idx[\"<PAD>\"] = len(tag2idx)  # Add PAD if needed\n",
        "tag2idx[\"<UNK>\"] = len(tag2idx)  # Optional: Add UNK tag\n",
        "\n",
        "print(\"Number of tags:\", len(tag2idx))\n",
        "print(tag2idx)\n"
      ],
      "metadata": {
        "id": "fnTYo5VJT5mf",
        "outputId": "2d2a8352-cc41-44e8-efad-ed8b5a0c0d1b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "fnTYo5VJT5mf",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tags: 27\n",
            "{'B-DATE': 0, 'B-LOC': 1, 'B-NUM': 2, 'B-ORG': 3, 'B-PER': 4, 'B-TIME': 5, 'E-DATE': 6, 'E-LOC': 7, 'E-NUM': 8, 'E-ORG': 9, 'E-PER': 10, 'E-TIME': 11, 'I-DATE': 12, 'I-LOC': 13, 'I-NUM': 14, 'I-ORG': 15, 'I-PER': 16, 'I-TIME': 17, 'O': 18, 'S-DATE': 19, 'S-LOC': 20, 'S-NUM': 21, 'S-ORG': 22, 'S-PER': 23, 'S-TIME': 24, '<PAD>': 25, '<UNK>': 26}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Batch max len:\", sentences.shape)\n",
        "    print(\"tags:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OWOoIDb5IIfc",
        "outputId": "be83ca5d-82fd-491b-c0bc-6f5837e632d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OWOoIDb5IIfc",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch max len: torch.Size([32, 41])\n",
            "tags: torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Force reinstall compatible versions\n",
        "# !pip install gensim\n",
        "# !pip install numpy==1.24.3 --force-reinstall"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0"
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(dataset):\n",
        "    word_set = set()\n",
        "    for sentence in dataset.sentences:\n",
        "        for word in sentence:\n",
        "            word_set.add(word)\n",
        "    return word_set\n",
        "\n",
        "vocab = build_vocab(train_data)\n",
        "vocab_size = len(vocab)\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "\n",
        "# Add PAD and UNK embeddings\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # <PAD>\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # <UNK>\n",
        "\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix).float()"
      ],
      "metadata": {
        "id": "l3PaPJLJLXs0"
      },
      "id": "l3PaPJLJLXs0",
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network with dropout and GELU activation.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Factor by which the hidden layer dimension will expand\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))  # Apply GELU and dropout on the first layer\n",
        "        return self.dropout2(self.fc2(x))  # Apply second linear layer with dropout\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer applying FFT twice (along two dimensions).\n",
        "\n",
        "    Parameters:\n",
        "    - dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFT along the last dimension (real and imaginary parts)\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)  # Apply FFT along the second dimension as well\n",
        "        x = self.act(x.real)  # Using only real part\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FNet model combining Fourier transformations and FeedForward layers.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Expansion factor for FeedForward layers\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout,embedding_matrix):\n",
        "\n",
        "        super(FNet, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Freeze embeddings\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        out = self.norm2(x + residual)\n",
        "        return out\n",
        "\n",
        "# Final NER model with classification head\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fnet(x)  # [batch_size, seq_len, dim]\n",
        "        logits = self.classifier(x)  # [batch_size, seq_len, num_tags]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(len(word2idx))\n",
        "print((len(tag2idx)))"
      ],
      "metadata": {
        "id": "688viprCUUux",
        "outputId": "94098631-d7ca-4c0e-be90-67b882555449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "688viprCUUux",
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16736, 300])\n",
            "16736\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "print(\"Number of tags:\", len(tag2idx))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOkqeu_1klTs",
        "outputId": "40300eb3-e6a7-4795-ff60-f9dd1295fe4b"
      },
      "id": "JOkqeu_1klTs",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tags: 27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNetNER(embedding_matrix, num_tags=len(tag2idx), expension_factor=4, dropout=0.3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = tag2idx[\"<PAD>\"])"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 25\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "\n",
        "    for sentences, tags in train_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * seq_len, num_tags]\n",
        "        tags = tags.view(-1)                           # [batch_size * seq_len]\n",
        "\n",
        "        loss = criterion(outputs, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_load)\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in val_load:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            loss = criterion(outputs, tags)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_load)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d02a6b7-6308-4a37-f419-311b548360ee"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Train Loss: 0.4597 - Val Loss: 0.3081\n",
            "Epoch 2/2 - Train Loss: 0.2740 - Val Loss: 0.2634\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test case\n",
        "test_sentence = [\"ရန်ကုန်\", \"မြို့\", \"သို့\", \"သွားသည်။\"]\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)  # [1, seq_len, num_tags]\n",
        "    predicted_ids = torch.argmax(output, dim=-1).squeeze(0).tolist()  # Remove batch dim\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "2iCktKo3WXx6",
        "outputId": "29631cce-f086-4537-cc5c-b8a2fc8e2bba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iCktKo3WXx6",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ရန်ကုန်\tB-DATE\n",
            "မြို့\tB-LOC\n",
            "သို့\tB-LOC\n",
            "သွားသည်။\tB-DATE\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}