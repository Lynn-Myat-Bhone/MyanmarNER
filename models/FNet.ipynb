{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "4150582d-9d7c-465d-ed88-d467ca102ab6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath, word2idx, tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # word2idx[\"<PAD>\"]\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "\n",
        "    return torch.tensor(padded_sentences, dtype=torch.long), torch.tensor(padded_tags, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_tags(filepath):\n",
        "    tag_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    tag = parts[2]\n",
        "                    tag_set.add(tag)\n",
        "    return sorted(tag_set)\n"
      ],
      "metadata": {
        "id": "3-4hXt0iT3eL"
      },
      "id": "3-4hXt0iT3eL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\""
      ],
      "metadata": {
        "id": "uhkHXmoz4Lhp"
      },
      "id": "uhkHXmoz4Lhp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Force reinstall compatible versions\n",
        "# !pip install gensim\n",
        "# !pip install numpy==1.24.3 --force-reinstall"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0"
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "LVhMJrOhqTbL",
      "metadata": {
        "id": "LVhMJrOhqTbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "163ed485-be97-4f55-ba1a-db4ab7301183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-d3a06eb518f2>:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  embedding_matrix = torch.tensor(embedding_matrix).float()\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Start with PAD and UNK\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "# Step 2: Load raw tokens from file before creating dataset\n",
        "def build_vocab_from_file(filepath, word2idx, tag2idx):\n",
        "    word_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    word, _, tag = parts\n",
        "                    word_set.add(word)\n",
        "                    if tag not in tag2idx:\n",
        "                        tag2idx[tag] = len(tag2idx)\n",
        "    return word_set\n",
        "\n",
        "# Build vocab from all datasets\n",
        "vocab = set()\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    vocab |= build_vocab_from_file(path, word2idx, tag2idx)\n",
        "\n",
        "# Step 3: Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # PAD\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # UNK\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix).float()\n",
        "\n",
        "# Step 4: Now it's safe to create datasets\n",
        "train_data = CustomDataset(train_path, word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path, word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path, word2idx, tag2idx)\n",
        "\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_tags = extract_unique_tags(train_path)\n",
        "# tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "# tag2idx[\"<PAD>\"] = len(tag2idx)  # Add PAD if needed\n",
        "# tag2idx[\"<UNK>\"] = len(tag2idx)  # Optional: Add UNK tag\n",
        "\n",
        "# print(\"Number of tags:\", len(tag2idx))\n",
        "# print(tag2idx)\n"
      ],
      "metadata": {
        "id": "fnTYo5VJT5mf"
      },
      "id": "fnTYo5VJT5mf",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of tags\n",
        "all_tags = [tag for seq in train_data.ner_tags for tag in seq]\n",
        "\n",
        "# Count the frequency of each tag\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "# Display the counts\n",
        "for tag, count in tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMbRy8Jnlfmq",
        "outputId": "cc65fdb7-9075-4eef-f4cb-0a42d716a058"
      },
      "id": "MMbRy8Jnlfmq",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-LOC: 9395\n",
            "I-LOC: 4015\n",
            "E-LOC: 9395\n",
            "O: 167547\n",
            "S-NUM: 3882\n",
            "B-DATE: 599\n",
            "I-DATE: 388\n",
            "E-DATE: 599\n",
            "S-PER: 1911\n",
            "S-LOC: 991\n",
            "S-DATE: 699\n",
            "B-ORG: 308\n",
            "E-ORG: 308\n",
            "S-ORG: 184\n",
            "I-ORG: 208\n",
            "B-PER: 281\n",
            "E-PER: 281\n",
            "B-TIME: 143\n",
            "E-TIME: 143\n",
            "B-NUM: 151\n",
            "E-NUM: 151\n",
            "S-TIME: 118\n",
            "I-TIME: 92\n",
            "I-NUM: 32\n",
            "I-PER: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Batch max len:\", sentences.shape)\n",
        "    print(\"tags:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OWOoIDb5IIfc",
        "outputId": "c24ea304-2fa9-4a24-8f25-1e9c52a5871b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OWOoIDb5IIfc",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch max len: torch.Size([32, 41])\n",
            "tags: torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network with dropout and GELU activation.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Factor by which the hidden layer dimension will expand\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))  # Apply GELU and dropout on the first layer\n",
        "        return self.dropout2(self.fc2(x))  # Apply second linear layer with dropout\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer applying FFT twice (along two dimensions).\n",
        "\n",
        "    Parameters:\n",
        "    - dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFT along the last dimension (real and imaginary parts)\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)  # Apply FFT along the second dimension as well\n",
        "        x = self.act(x.real)  # Using only real part\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FNet model combining Fourier transformations and FeedForward layers.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Expansion factor for FeedForward layers\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout,embedding_matrix):\n",
        "\n",
        "        super(FNet, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Freeze embeddings\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        out = self.norm2(x + residual)\n",
        "        return out\n",
        "\n",
        "# Final NER model with classification head\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fnet(x)  # [batch_size, seq_len, dim]\n",
        "        logits = self.classifier(x)  # [batch_size, seq_len, num_tags]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(len(word2idx))\n",
        "print((len(tag2idx)))"
      ],
      "metadata": {
        "id": "688viprCUUux",
        "outputId": "821cdd5c-4f9b-4987-e880-5576068eb1fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "688viprCUUux",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19304, 300])\n",
            "19304\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNetNER(embedding_matrix, num_tags=len(tag2idx), expension_factor=4, dropout=0.3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4)\n"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = []\n",
        "for tag_seq in train_data.ner_tags:\n",
        "    labels.extend(tag_seq)\n",
        "\n",
        "\n",
        "label_ids = [tag2idx[tag] for tag in labels]\n",
        "present_classes = np.unique(label_ids)\n",
        "present_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=label_ids)\n",
        "\n",
        "full_weights = np.ones(len(tag2idx))\n",
        "for i, cls in enumerate(present_classes):\n",
        "    full_weights[cls] = present_weights[i]\n",
        "\n",
        "weights = torch.tensor(full_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"], weight=weights)\n"
      ],
      "metadata": {
        "id": "DJN2lrE_uKAV"
      },
      "id": "DJN2lrE_uKAV",
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "present_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XD4YVpq0Itf",
        "outputId": "7903aa44-f8ad-4ee8-ba02-fc51a07dc1f1"
      },
      "id": "2XD4YVpq0Itf",
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "       19, 20, 21, 22, 23, 24, 25, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for sentences, tags in train_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * seq_len, num_tags]\n",
        "        tags = tags.view(-1)                           # [batch_size * seq_len]\n",
        "\n",
        "        loss = criterion(outputs, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_load)\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in val_load:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            loss = criterion(outputs, tags)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_load)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b047aa-2131-4507-99e7-eb0cfc0e1a35"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Train Loss: 1.2452 - Val Loss: 3.1101\n",
            "Epoch 2/2 - Train Loss: 0.9842 - Val Loss: 3.1569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test case\n",
        "test_sentence = [\"ရန်\",\"ကုန်\", \"မြို့\", \"သို့\", \"သွား\",\"သည်။\"]\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)  # [1, seq_len, num_tags]\n",
        "    predicted_ids = torch.argmax(output, dim=-1).squeeze(0).tolist()  # Remove batch dim\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "2iCktKo3WXx6",
        "outputId": "0334f048-84db-4419-ff31-b72340cfb5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iCktKo3WXx6",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ရန်\tO\n",
            "ကုန်\tO\n",
            "မြို့\tO\n",
            "သို့\tO\n",
            "သွား\tO\n",
            "သည်။\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's parameters\n",
        "torch.save(model.state_dict(), \"ner_model.pth\")"
      ],
      "metadata": {
        "id": "zekIuzU_BSry"
      },
      "id": "zekIuzU_BSry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "WtsFBuwgcU9r"
      },
      "id": "WtsFBuwgcU9r"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}