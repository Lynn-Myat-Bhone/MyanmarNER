{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "aa097e07-d0ff-4247-8fd3-ffb4000a1c86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath, word2idx, tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # word2idx[\"<PAD>\"]\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "\n",
        "    return torch.tensor(padded_sentences, dtype=torch.long), torch.tensor(padded_tags, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_tags(filepath):\n",
        "    tag_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    tag = parts[2]\n",
        "                    tag_set.add(tag)\n",
        "    return sorted(tag_set)\n"
      ],
      "metadata": {
        "id": "3-4hXt0iT3eL"
      },
      "id": "3-4hXt0iT3eL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\""
      ],
      "metadata": {
        "id": "uhkHXmoz4Lhp"
      },
      "id": "uhkHXmoz4Lhp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Force reinstall compatible versions\n",
        "# !pip install gensim\n",
        "# !pip install numpy==1.24.3 --force-reinstall"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0",
        "outputId": "86b32dfe-fed4-41b6-f303-30cebf8cf55c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
            "Collecting numpy==1.24.3\n",
            "  Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m95.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.3 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.24.3 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.3 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.24.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.3 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "9ee72e34b0ef462fb1a73b440576f246"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "LVhMJrOhqTbL",
      "metadata": {
        "id": "LVhMJrOhqTbL",
        "outputId": "78bcd27e-4cd2-4840-98ec-ffbbd1bd658a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-8cb3e15e79f0>:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  embedding_matrix = torch.tensor(embedding_matrix).float()\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Start with PAD and UNK\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "# Step 2: Load raw tokens from file before creating dataset\n",
        "def build_vocab_from_file(filepath, word2idx, tag2idx):\n",
        "    word_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    word, _, tag = parts\n",
        "                    word_set.add(word)\n",
        "                    if tag not in tag2idx:\n",
        "                        tag2idx[tag] = len(tag2idx)\n",
        "    return word_set\n",
        "\n",
        "# Build vocab from all datasets\n",
        "vocab = set()\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    vocab |= build_vocab_from_file(path, word2idx, tag2idx)\n",
        "\n",
        "# Step 3: Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # PAD\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # UNK\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix).float()\n",
        "\n",
        "# Step 4: Now it's safe to create datasets\n",
        "train_data = CustomDataset(train_path, word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path, word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path, word2idx, tag2idx)\n",
        "\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32,shuffle = True,collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32,shuffle = False,collate_fn=collate_fn)\n",
        "test_load = DataLoader(test_data, batch_size=32,shuffle = False,collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of tags\n",
        "all_tags = [tag for seq in train_data.ner_tags for tag in seq]\n",
        "\n",
        "# Count the frequency of each tag\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "# Display the counts\n",
        "for tag, count in tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMbRy8Jnlfmq",
        "outputId": "f323d8cd-16da-4f35-8ab2-9ab7c25c4479"
      },
      "id": "MMbRy8Jnlfmq",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-LOC: 9395\n",
            "I-LOC: 4015\n",
            "E-LOC: 9395\n",
            "O: 167547\n",
            "S-NUM: 3882\n",
            "B-DATE: 599\n",
            "I-DATE: 388\n",
            "E-DATE: 599\n",
            "S-PER: 1911\n",
            "S-LOC: 991\n",
            "S-DATE: 699\n",
            "B-ORG: 308\n",
            "E-ORG: 308\n",
            "S-ORG: 184\n",
            "I-ORG: 208\n",
            "B-PER: 281\n",
            "E-PER: 281\n",
            "B-TIME: 143\n",
            "E-TIME: 143\n",
            "B-NUM: 151\n",
            "E-NUM: 151\n",
            "S-TIME: 118\n",
            "I-TIME: 92\n",
            "I-NUM: 32\n",
            "I-PER: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Batch max len:\", sentences.shape)\n",
        "    print(\"tags:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OWOoIDb5IIfc",
        "outputId": "9441d29e-2d9c-4a73-d464-dcfbfdcf6e15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OWOoIDb5IIfc",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch max len: torch.Size([32, 34])\n",
            "tags: torch.Size([32, 34])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,max_len,d_model):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len,d_model)\n",
        "        position = torch.arange(max_len).unsqueeze(1) # shape[max_len,1]\n",
        "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *-(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        self.register_buffer('pe', pe)  # Register the positional encoding tensor as a buffer\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x (batch_size,max_len, d_model)\n",
        "        x = x + self.pe[:x.size(1)] #BroadCast positional encoding\n",
        "        return x"
      ],
      "metadata": {
        "id": "QEbeIbS5tua8"
      },
      "id": "QEbeIbS5tua8",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network with dropout and GELU activation.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Factor by which the hidden layer dimension will expand\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))  # Apply GELU and dropout on the first layer\n",
        "        return self.dropout2(self.fc2(x))  # Apply second linear layer with dropout\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer applying FFT twice (along two dimensions).\n",
        "\n",
        "    Parameters:\n",
        "    - dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFT along the last dimension (real and imaginary parts)\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)  # Apply FFT along the second dimension as well\n",
        "        x = self.act(x.real)  # Using only real part\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FNet model combining Fourier transformations and FeedForward layers.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Expansion factor for FeedForward layers\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout,embedding_matrix):\n",
        "\n",
        "        super(FNet, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Freeze embeddings\n",
        "        self.pos_en = PositionalEncoding(512, d_model = dim)\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_en(x)\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        out = self.norm2(x + residual)\n",
        "        return out\n",
        "\n",
        "# Final NER model with classification head\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fnet(x)  # [batch_size, seq_len, dim]\n",
        "        logits = self.classifier(x)  # [batch_size, seq_len, num_tags]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(len(word2idx))\n",
        "print((len(tag2idx)))"
      ],
      "metadata": {
        "id": "688viprCUUux",
        "outputId": "ece31a85-ab26-43d0-e94a-ae0543fa557e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "688viprCUUux",
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19304, 300])\n",
            "19304\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNetNER(embedding_matrix, num_tags=len(tag2idx), expension_factor=2, dropout=0.3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4,weight_decay=1e-5)\n"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = []\n",
        "for tag_seq in train_data.ner_tags:\n",
        "    labels.extend(tag_seq)\n",
        "\n",
        "\n",
        "label_ids = [tag2idx[tag] for tag in labels]\n",
        "present_classes = np.unique(label_ids)\n",
        "present_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=label_ids)\n",
        "\n",
        "full_weights = np.ones(len(tag2idx))\n",
        "for i, cls in enumerate(present_classes):\n",
        "    full_weights[cls] = present_weights[i]\n",
        "\n",
        "weights = torch.tensor(full_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"], weight=weights)\n"
      ],
      "metadata": {
        "id": "DJN2lrE_uKAV"
      },
      "id": "DJN2lrE_uKAV",
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 50\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for sentences, tags in train_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * seq_len, num_tags]\n",
        "        tags = tags.view(-1)                           # [batch_size * seq_len]\n",
        "\n",
        "        loss = criterion(outputs, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_load)\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in val_load:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            loss = criterion(outputs, tags)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_load)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0830f5b-61a5-4465-c049-cdd0e9056404"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 2.9270 - Val Loss: 2.5941\n",
            "Epoch 2/10 - Train Loss: 2.6011 - Val Loss: 2.6141\n",
            "Epoch 3/10 - Train Loss: 2.3805 - Val Loss: 2.5009\n",
            "Epoch 4/10 - Train Loss: 2.2723 - Val Loss: 2.3990\n",
            "Epoch 5/10 - Train Loss: 2.1743 - Val Loss: 2.4142\n",
            "Epoch 6/10 - Train Loss: 2.0801 - Val Loss: 2.3069\n",
            "Epoch 7/10 - Train Loss: 1.9978 - Val Loss: 2.3925\n",
            "Epoch 8/10 - Train Loss: 1.8933 - Val Loss: 2.2898\n",
            "Epoch 9/10 - Train Loss: 1.8610 - Val Loss: 2.3239\n",
            "Epoch 10/10 - Train Loss: 1.7547 - Val Loss: 2.2332\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test case\n",
        "test_sentence = [\"ကြက်ခြေနီ\",\"မှ\", \"ပြော\",\"ရေး\",\"ဆို\",\"ခွင့်\",\"ရှိ\",\"သူ\", \"MattCochrane\",\"သည်။\"]\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)  # [1, seq_len, num_tags]\n",
        "    predicted_ids = torch.argmax(output, dim=-1).squeeze(0).tolist()  # Remove batch dim\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "2iCktKo3WXx6",
        "outputId": "9be22e19-b9e9-4757-94f6-60641e9a13c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iCktKo3WXx6",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ကြက်ခြေနီ\tB-DATE\n",
            "မှ\tE-DATE\n",
            "ပြော\tS-PER\n",
            "ရေး\tB-DATE\n",
            "ဆို\tE-DATE\n",
            "ခွင့်\tE-DATE\n",
            "ရှိ\tE-DATE\n",
            "သူ\tB-ORG\n",
            "MattCochrane\tE-DATE\n",
            "သည်။\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's parameters\n",
        "torch.save(model.state_dict(), \"ner_model.pth\")"
      ],
      "metadata": {
        "id": "zekIuzU_BSry"
      },
      "id": "zekIuzU_BSry",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def evaluate_model(model, dataloader, idx2tag, pad_idx, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Create mask on the fly\n",
        "            mask = (inputs != pad_idx)\n",
        "\n",
        "            logits = model(inputs)\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            for i in range(inputs.size(0)):\n",
        "                true = labels[i][mask[i]].cpu().tolist()\n",
        "                pred = preds[i][mask[i]].cpu().tolist()\n",
        "\n",
        "                all_labels.extend([idx2tag[t] for t in true])\n",
        "                all_preds.extend([idx2tag[p] for p in pred])\n",
        "\n",
        "    print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "\n",
        "pad_idx = word2idx[\"<PAD>\"]\n",
        "evaluate_model(model, test_load, idx2tag, pad_idx, device)\n"
      ],
      "metadata": {
        "id": "tbyIKjRmTC2W",
        "outputId": "4a3bd62b-3c20-4e65-bfac-314d75f1935b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tbyIKjRmTC2W",
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      B-DATE     0.0301    0.2273    0.0532        66\n",
            "       B-LOC     0.4880    0.4628    0.4750      1182\n",
            "       B-NUM     0.0080    0.1333    0.0152        15\n",
            "       B-ORG     0.0177    0.1458    0.0316        48\n",
            "       B-PER     0.0097    0.0882    0.0174        34\n",
            "      B-TIME     0.0110    0.3333    0.0214         9\n",
            "      E-DATE     0.0295    0.3182    0.0539        66\n",
            "       E-LOC     0.2705    0.5118    0.3539      1182\n",
            "       E-NUM     0.0000    0.0000    0.0000        15\n",
            "       E-ORG     0.0101    0.1042    0.0185        48\n",
            "       E-PER     0.0158    0.3824    0.0304        34\n",
            "      E-TIME     0.0039    0.6667    0.0078         9\n",
            "      I-DATE     0.0000    0.0000    0.0000        38\n",
            "       I-LOC     0.2410    0.8131    0.3718       503\n",
            "       I-NUM     0.0000    0.0000    0.0000         0\n",
            "       I-ORG     0.0019    0.0256    0.0036        39\n",
            "       I-PER     0.0000    0.0000    0.0000         0\n",
            "      I-TIME     0.0000    0.0000    0.0000         0\n",
            "           O     0.9697    0.2762    0.4299     21324\n",
            "      S-DATE     0.3820    0.7727    0.5113        88\n",
            "       S-LOC     0.0197    0.1855    0.0357       124\n",
            "       S-NUM     0.1694    0.7389    0.2756       475\n",
            "       S-ORG     0.0000    0.0000    0.0000        21\n",
            "       S-PER     0.0567    0.5650    0.1031       223\n",
            "      S-TIME     0.0000    0.0000    0.0000         0\n",
            "\n",
            "    accuracy                         0.3169     25543\n",
            "   macro avg     0.1094    0.2700    0.1124     25543\n",
            "weighted avg     0.8547    0.3169    0.4130     25543\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "WtsFBuwgcU9r"
      },
      "id": "WtsFBuwgcU9r"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}