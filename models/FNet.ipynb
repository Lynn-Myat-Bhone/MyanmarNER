{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "de689703-e225-44bc-b9c8-549d26f2c780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "N-_RXQTAqWhl",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ebc74ea7",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "ebc74ea7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self,filepath,word2idx,tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences , self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self,filepath):\n",
        "        sentences, ner_tags = [],[]\n",
        "        sentence, ner_tag = [],[]\n",
        "        with open(filepath,\"r\",encoding = \"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    word,_,ner = line.split(\"\\t\")\n",
        "                    sentence.append(word)\n",
        "                    ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []  # rest after every sentence\n",
        "            return sentences,ner_tags\n",
        "    def __len__(self):\n",
        "      return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "  sentences,tag = zip(*batch)\n",
        "  max_len = max(len(s) for s in sentences)\n",
        "  padded_sentences = [  s+ [0] * (max_len-len(s)) for s in sentences]\n",
        "  padded_tag = [ t + [0] *(max_len-len(t)) for t in tag]\n",
        "\n",
        "  return torch.tensor(padded_sentences),torch.tensor(padded_tag)"
      ],
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "id": "1DUeTjUbqMXk",
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Dataset"
      ],
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "id": "VP9MDl5CshWD"
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\"\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "train_data = CustomDataset(train_path,word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path,word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path,word2idx, tag2idx)\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n",
        "test_load = DataLoader(test_data, batch_size=32, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "LVhMJrOhqTbL"
      },
      "id": "LVhMJrOhqTbL",
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Sentences shape:\", sentences.shape)\n",
        "    print(\"Tags shape:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "hdakweU15BWA",
        "outputId": "1ccb95c0-4ffa-44f6-cc51-18a6ba3268f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "hdakweU15BWA",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences shape: torch.Size([32, 41])\n",
            "Tags shape: torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pretrained with  fasttext HERE"
      ],
      "metadata": {
        "id": "s03nfQnkIFyv"
      },
      "id": "s03nfQnkIFyv",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}