{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "dfbf4a0d-a286-4793-cb80-6e6b113bd055",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath, word2idx, tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # word2idx[\"<PAD>\"]\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "\n",
        "    return torch.tensor(padded_sentences, dtype=torch.long), torch.tensor(padded_tags, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_tags(filepath):\n",
        "    tag_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    tag = parts[2]\n",
        "                    tag_set.add(tag)\n",
        "    return sorted(tag_set)\n"
      ],
      "metadata": {
        "id": "3-4hXt0iT3eL"
      },
      "id": "3-4hXt0iT3eL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/MyanmarNER/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/MyanmarNER/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/MyanmarNER/test_v5.conll\""
      ],
      "metadata": {
        "id": "uhkHXmoz4Lhp"
      },
      "id": "uhkHXmoz4Lhp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Force reinstall compatible versions\n",
        "!pip uninstall -y numpy gensim scipy\n",
        "!pip install numpy==1.23.5\n",
        "!pip install scipy==1.10.1\n",
        "!pip install gensim==4.3.1\n",
        "\n"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6c88171d-ff06-406c-ae93-a7e654809ac0"
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "\u001b[33mWARNING: Skipping gensim as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.23.5\n",
            "  Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
            "Using cached numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "miniful 0.0.6 requires scipy>=1.0.0, which is not installed.\n",
            "pyfume 0.3.1 requires scipy, which is not installed.\n",
            "fuzzytm 2.0.9 requires scipy, which is not installed.\n",
            "simpful 2.12.0 requires scipy>=1.0.0, which is not installed.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, which is not installed.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, which is not installed.\n",
            "jax 0.5.2 requires scipy>=1.11.1, which is not installed.\n",
            "mizani 0.13.5 requires scipy>=1.8.0, which is not installed.\n",
            "cuml-cu12 25.2.1 requires scipy>=1.8.0, which is not installed.\n",
            "mlxtend 0.23.4 requires scipy>=1.2.1, which is not installed.\n",
            "arviz 0.21.0 requires scipy>=1.9.0, which is not installed.\n",
            "lightgbm 4.5.0 requires scipy, which is not installed.\n",
            "stumpy 1.13.0 requires scipy>=1.10, which is not installed.\n",
            "scs 3.2.7.post2 requires scipy, which is not installed.\n",
            "missingno 0.5.2 requires scipy, which is not installed.\n",
            "umap-learn 0.5.7 requires scipy>=1.3.1, which is not installed.\n",
            "xgboost 2.1.4 requires scipy, which is not installed.\n",
            "shap 0.47.2 requires scipy, which is not installed.\n",
            "fastai 2.7.19 requires scipy, which is not installed.\n",
            "pytensor 2.30.3 requires scipy<2,>=1, which is not installed.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, which is not installed.\n",
            "matplotlib-venn 1.1.2 requires scipy, which is not installed.\n",
            "hyperopt 0.2.7 requires scipy, which is not installed.\n",
            "pymc 5.22.0 requires scipy>=1.4.1, which is not installed.\n",
            "clarabel 0.10.0 requires scipy, which is not installed.\n",
            "hdbscan 0.8.40 requires scipy>=1.0, which is not installed.\n",
            "yellowbrick 1.5 requires scipy>=1.0.0, which is not installed.\n",
            "treelite 4.4.1 requires scipy, which is not installed.\n",
            "imbalanced-learn 0.13.0 requires scipy<2,>=1.10.1, which is not installed.\n",
            "libpysal 4.13.0 requires scipy>=1.8, which is not installed.\n",
            "librosa 0.11.0 requires scipy>=1.6.0, which is not installed.\n",
            "albumentations 2.0.6 requires scipy>=1.10.0, which is not installed.\n",
            "plotnine 0.14.5 requires scipy>=1.8.0, which is not installed.\n",
            "datascience 0.17.6 requires scipy, which is not installed.\n",
            "statsmodels 0.14.4 requires scipy!=1.9.2,>=1.8, which is not installed.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", which is not installed.\n",
            "sklearn-pandas 2.2.0 requires scipy>=1.5.1, which is not installed.\n",
            "scikit-learn 1.6.1 requires scipy>=1.6.0, which is not installed.\n",
            "osqp 1.0.4 requires scipy>=0.13.2, which is not installed.\n",
            "xarray-einstats 0.8.0 requires scipy>=1.9, which is not installed.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.3.1 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "bigframes 2.4.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "blosc2 3.3.2 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "albucore 0.0.24 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "db-dtypes 1.4.3 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.23.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "4c862c5d9e3d46ef8353db0444a3b811"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scipy==1.10.1\n",
            "  Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Requirement already satisfied: numpy<1.27.0,>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scipy==1.10.1) (1.23.5)\n",
            "Using cached scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n",
            "Installing collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cvxpy 1.6.5 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "pymc 5.22.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.6 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed scipy-1.10.1\n",
            "Collecting gensim==4.3.1\n",
            "  Downloading gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.1) (1.17.2)\n",
            "Downloading gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/MyanmarNER/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LVhMJrOhqTbL",
      "metadata": {
        "id": "LVhMJrOhqTbL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a93725d7-9bc9-4aa4-ba54-546ee9b1967e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-d3a06eb518f2>:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n",
            "  embedding_matrix = torch.tensor(embedding_matrix).float()\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Start with PAD and UNK\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "# Step 2: Load raw tokens from file before creating dataset\n",
        "def build_vocab_from_file(filepath, word2idx, tag2idx):\n",
        "    word_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    word, _, tag = parts\n",
        "                    word_set.add(word)\n",
        "                    if tag not in tag2idx:\n",
        "                        tag2idx[tag] = len(tag2idx)\n",
        "    return word_set\n",
        "\n",
        "# Build vocab from all datasets\n",
        "vocab = set()\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    vocab |= build_vocab_from_file(path, word2idx, tag2idx)\n",
        "\n",
        "# Step 3: Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # PAD\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # UNK\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix).float()\n",
        "\n",
        "# Step 4: Now it's safe to create datasets\n",
        "train_data = CustomDataset(train_path, word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path, word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path, word2idx, tag2idx)\n",
        "\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_tags = extract_unique_tags(train_path)\n",
        "# tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "# tag2idx[\"<PAD>\"] = len(tag2idx)  # Add PAD if needed\n",
        "# tag2idx[\"<UNK>\"] = len(tag2idx)  # Optional: Add UNK tag\n",
        "\n",
        "# print(\"Number of tags:\", len(tag2idx))\n",
        "# print(tag2idx)\n"
      ],
      "metadata": {
        "id": "fnTYo5VJT5mf"
      },
      "id": "fnTYo5VJT5mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of tags\n",
        "all_tags = [tag for seq in train_data.ner_tags for tag in seq]\n",
        "\n",
        "# Count the frequency of each tag\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "# Display the counts\n",
        "for tag, count in tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMbRy8Jnlfmq",
        "outputId": "91708698-0d13-4917-bfeb-fd9b117f4d91"
      },
      "id": "MMbRy8Jnlfmq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-LOC: 9395\n",
            "I-LOC: 4015\n",
            "E-LOC: 9395\n",
            "O: 167547\n",
            "S-NUM: 3882\n",
            "B-DATE: 599\n",
            "I-DATE: 388\n",
            "E-DATE: 599\n",
            "S-PER: 1911\n",
            "S-LOC: 991\n",
            "S-DATE: 699\n",
            "B-ORG: 308\n",
            "E-ORG: 308\n",
            "S-ORG: 184\n",
            "I-ORG: 208\n",
            "B-PER: 281\n",
            "E-PER: 281\n",
            "B-TIME: 143\n",
            "E-TIME: 143\n",
            "B-NUM: 151\n",
            "E-NUM: 151\n",
            "S-TIME: 118\n",
            "I-TIME: 92\n",
            "I-NUM: 32\n",
            "I-PER: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Batch max len:\", sentences.shape)\n",
        "    print(\"tags:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OWOoIDb5IIfc",
        "outputId": "c4f5afbc-3731-4574-f367-1b28eeafb19d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OWOoIDb5IIfc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch max len: torch.Size([32, 41])\n",
            "tags: torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network with dropout and GELU activation.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Factor by which the hidden layer dimension will expand\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))  # Apply GELU and dropout on the first layer\n",
        "        return self.dropout2(self.fc2(x))  # Apply second linear layer with dropout\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer applying FFT twice (along two dimensions).\n",
        "\n",
        "    Parameters:\n",
        "    - dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFT along the last dimension (real and imaginary parts)\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)  # Apply FFT along the second dimension as well\n",
        "        x = self.act(x.real)  # Using only real part\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FNet model combining Fourier transformations and FeedForward layers.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Expansion factor for FeedForward layers\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout,embedding_matrix):\n",
        "\n",
        "        super(FNet, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Freeze embeddings\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        out = self.norm2(x + residual)\n",
        "        return out\n",
        "\n",
        "# Final NER model with classification head\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fnet(x)  # [batch_size, seq_len, dim]\n",
        "        logits = self.classifier(x)  # [batch_size, seq_len, num_tags]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(len(word2idx))\n",
        "print((len(tag2idx)))"
      ],
      "metadata": {
        "id": "688viprCUUux",
        "outputId": "c0331504-207b-4a80-a2cd-1068a1f98e62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "688viprCUUux",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19304, 300])\n",
            "19304\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNetNER(embedding_matrix, num_tags=len(tag2idx), expension_factor=4, dropout=0.3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4)\n"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = []\n",
        "for tag_seq in train_data.ner_tags:\n",
        "    labels.extend(tag_seq)\n",
        "\n",
        "\n",
        "label_ids = [tag2idx[tag] for tag in labels]\n",
        "present_classes = np.unique(label_ids)\n",
        "present_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=label_ids)\n",
        "\n",
        "full_weights = np.ones(len(tag2idx))\n",
        "for i, cls in enumerate(present_classes):\n",
        "    full_weights[cls] = present_weights[i]\n",
        "\n",
        "weights = torch.tensor(full_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"], weight=weights)\n"
      ],
      "metadata": {
        "id": "DJN2lrE_uKAV"
      },
      "id": "DJN2lrE_uKAV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "present_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XD4YVpq0Itf",
        "outputId": "d8dc2749-82c4-4fa1-da52-5c3148e67b75"
      },
      "id": "2XD4YVpq0Itf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n",
              "       19, 20, 21, 22, 23, 24, 25, 26])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 2\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for sentences, tags in train_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * seq_len, num_tags]\n",
        "        tags = tags.view(-1)                           # [batch_size * seq_len]\n",
        "\n",
        "        loss = criterion(outputs, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_load)\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in val_load:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            loss = criterion(outputs, tags)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_load)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b440dac0-9a9a-4258-e130-ac5973fde918"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2 - Train Loss: 1.7140 - Val Loss: 3.5384\n",
            "Epoch 2/2 - Train Loss: 1.4876 - Val Loss: 3.1203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test case\n",
        "test_sentence = [\"ရန်\",\"ကုန်\", \"မြို့\", \"သို့\", \"သွား\",\"သည်။\"]\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)  # [1, seq_len, num_tags]\n",
        "    predicted_ids = torch.argmax(output, dim=-1).squeeze(0).tolist()  # Remove batch dim\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "2iCktKo3WXx6",
        "outputId": "94b68f48-3f6d-49bb-e7aa-077c1e8bea50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iCktKo3WXx6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ရန်\tO\n",
            "ကုန်\tO\n",
            "မြို့\tO\n",
            "သို့\tO\n",
            "သွား\tO\n",
            "သည်။\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's parameters\n",
        "torch.save(model.state_dict(), \"ner_model.pth\")"
      ],
      "metadata": {
        "id": "zekIuzU_BSry"
      },
      "id": "zekIuzU_BSry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "WtsFBuwgcU9r"
      },
      "id": "WtsFBuwgcU9r"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test Evaluation"
      ],
      "metadata": {
        "id": "94v72yZ1FiOf"
      },
      "id": "94v72yZ1FiOf"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_load = DataLoader(test_data, batch_size=32, collate_fn=collate_fn)\n",
        "\n",
        "model.eval()\n",
        "correct_predictions = 0\n",
        "total_predictions = 0\n",
        "true_positives = {}\n",
        "false_positives = {}\n",
        "false_negatives = {}\n",
        "\n",
        "for tag in tag2idx.keys():\n",
        "    true_positives[tag] = 0\n",
        "    false_positives[tag] = 0\n",
        "    false_negatives[tag] = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sentences, tags in test_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        predicted_ids = torch.argmax(outputs, dim=-1)\n",
        "\n",
        "        # Flatten predictions and true tags, excluding padding\n",
        "        mask = (tags != tag2idx[\"<PAD>\"])\n",
        "        predicted_ids_flat = predicted_ids[mask].cpu().numpy()\n",
        "        tags_flat = tags[mask].cpu().numpy()\n",
        "\n",
        "        correct_predictions += (predicted_ids_flat == tags_flat).sum()\n",
        "        total_predictions += len(tags_flat)\n",
        "\n",
        "        # Calculate TP, FP, FN for each tag\n",
        "        for i in range(len(tags_flat)):\n",
        "            true_tag_id = tags_flat[i]\n",
        "            predicted_tag_id = predicted_ids_flat[i]\n",
        "            true_tag = idx2tag[true_tag_id]\n",
        "            predicted_tag = idx2tag[predicted_tag_id]\n",
        "\n",
        "            if true_tag_id == predicted_tag_id:\n",
        "                true_positives[true_tag] += 1\n",
        "            else:\n",
        "                false_positives[predicted_tag] += 1\n",
        "                false_negatives[true_tag] += 1\n",
        "\n",
        "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Calculate precision, recall, and F1-score for each tag\n",
        "print(\"\\nPerformance per tag:\")\n",
        "for tag in sorted(tag2idx.keys()):\n",
        "    if tag == \"<PAD>\" or tag == \"<UNK>\":\n",
        "        continue\n",
        "\n",
        "    tp = true_positives[tag]\n",
        "    fp = false_positives[tag]\n",
        "    fn = false_negatives[tag]\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    print(f\"Tag: {tag}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "WmnyxdnUFkGT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a42adc3-a03f-45c6-dad5-f4f001793745"
      },
      "id": "WmnyxdnUFkGT",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Accuracy: 0.8351\n",
            "\n",
            "Performance per tag:\n",
            "Tag: B-DATE\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: B-LOC\n",
            "  Precision: 0.3333\n",
            "  Recall: 0.0051\n",
            "  F1-Score: 0.0100\n",
            "Tag: B-NUM\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: B-ORG\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: B-PER\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: B-TIME\n",
            "  Precision: 0.2500\n",
            "  Recall: 0.1111\n",
            "  F1-Score: 0.1538\n",
            "Tag: E-DATE\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: E-LOC\n",
            "  Precision: 0.2105\n",
            "  Recall: 0.0034\n",
            "  F1-Score: 0.0067\n",
            "Tag: E-NUM\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: E-ORG\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: E-PER\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: E-TIME\n",
            "  Precision: 0.1786\n",
            "  Recall: 0.5556\n",
            "  F1-Score: 0.2703\n",
            "Tag: I-DATE\n",
            "  Precision: 0.4000\n",
            "  Recall: 0.0526\n",
            "  F1-Score: 0.0930\n",
            "Tag: I-LOC\n",
            "  Precision: 0.2973\n",
            "  Recall: 0.0219\n",
            "  F1-Score: 0.0407\n",
            "Tag: I-NUM\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: I-ORG\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: I-PER\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: I-TIME\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: O\n",
            "  Precision: 0.8384\n",
            "  Recall: 0.9955\n",
            "  F1-Score: 0.9102\n",
            "Tag: S-DATE\n",
            "  Precision: 0.9848\n",
            "  Recall: 0.7386\n",
            "  F1-Score: 0.8442\n",
            "Tag: S-LOC\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: S-NUM\n",
            "  Precision: 0.2195\n",
            "  Recall: 0.0189\n",
            "  F1-Score: 0.0349\n",
            "Tag: S-ORG\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: S-PER\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n",
            "Tag: S-TIME\n",
            "  Precision: 0.0000\n",
            "  Recall: 0.0000\n",
            "  F1-Score: 0.0000\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}