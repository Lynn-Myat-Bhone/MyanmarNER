{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "N-_RXQTAqWhl",
      "metadata": {
        "id": "N-_RXQTAqWhl",
        "outputId": "b5b88df8-3434-4838-af5d-f5ddaf8e776c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ebc74ea7",
      "metadata": {
        "id": "ebc74ea7",
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1DUeTjUbqMXk",
      "metadata": {
        "id": "1DUeTjUbqMXk"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, filepath, word2idx, tag2idx):\n",
        "        self.word2idx = word2idx\n",
        "        self.tag2idx = tag2idx\n",
        "        self.sentences, self.ner_tags = self.load_data(filepath)\n",
        "\n",
        "    def load_data(self, filepath):\n",
        "        sentences, ner_tags = [], []\n",
        "        sentence, ner_tag = [], []\n",
        "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                if line:\n",
        "                    parts = line.split(\"\\t\")\n",
        "                    if len(parts) >= 3:\n",
        "                        word, _, ner = parts\n",
        "                        sentence.append(word)\n",
        "                        ner_tag.append(ner)\n",
        "                else:\n",
        "                    if sentence:\n",
        "                        sentences.append(sentence)\n",
        "                        ner_tags.append(ner_tag)\n",
        "                        sentence, ner_tag = [], []\n",
        "            # In case file does not end with a newline\n",
        "            if sentence:\n",
        "                sentences.append(sentence)\n",
        "                ner_tags.append(ner_tag)\n",
        "        return sentences, ner_tags\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        word_ids = [self.word2idx.get(w, self.word2idx[\"<UNK>\"]) for w in self.sentences[idx]]\n",
        "        tag_ids = [self.tag2idx.get(t, self.tag2idx[\"<UNK>\"]) for t in self.ner_tags[idx]]\n",
        "        return word_ids, tag_ids\n",
        "\n",
        "def collate_fn(batch):\n",
        "    sentences, tags = zip(*batch)\n",
        "    max_len = max(len(s) for s in sentences)\n",
        "\n",
        "    pad_token = 0  # word2idx[\"<PAD>\"]\n",
        "    padded_sentences = [s + [pad_token] * (max_len - len(s)) for s in sentences]\n",
        "    padded_tags = [t + [pad_token] * (max_len - len(t)) for t in tags]\n",
        "\n",
        "    return torch.tensor(padded_sentences, dtype=torch.long), torch.tensor(padded_tags, dtype=torch.long)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_unique_tags(filepath):\n",
        "    tag_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    tag = parts[2]\n",
        "                    tag_set.add(tag)\n",
        "    return sorted(tag_set)\n"
      ],
      "metadata": {
        "id": "3-4hXt0iT3eL"
      },
      "id": "3-4hXt0iT3eL",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = \"/content/drive/MyDrive/Datasets/train_v5.conll\"\n",
        "val_path = \"/content/drive/MyDrive/Datasets/val_v5.conll\"\n",
        "test_path = \"/content/drive/MyDrive/Datasets/test_v5.conll\""
      ],
      "metadata": {
        "id": "uhkHXmoz4Lhp"
      },
      "id": "uhkHXmoz4Lhp",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "VP9MDl5CshWD",
      "metadata": {
        "id": "VP9MDl5CshWD"
      },
      "source": [
        "Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# #Force reinstall compatible versions\n",
        "# !pip install gensim\n",
        "# !pip install numpy==1.24.3 --force-reinstall"
      ],
      "metadata": {
        "id": "uw8kNsdQJfi0"
      },
      "id": "uw8kNsdQJfi0",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('/content/drive/MyDrive/Datasets/cc.my.300.vec', binary=False)\n",
        "# https://fasttext.cc/docs/en/crawl-vectors.html choose Burmese choose text .vec file"
      ],
      "metadata": {
        "id": "dQexvLD-JDmp"
      },
      "id": "dQexvLD-JDmp",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "LVhMJrOhqTbL",
      "metadata": {
        "id": "LVhMJrOhqTbL"
      },
      "outputs": [],
      "source": [
        "# Step 1: Start with PAD and UNK\n",
        "word2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "tag2idx = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
        "\n",
        "# Step 2: Load raw tokens from file before creating dataset\n",
        "def build_vocab_from_file(filepath, word2idx, tag2idx):\n",
        "    word_set = set()\n",
        "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                parts = line.split(\"\\t\")\n",
        "                if len(parts) >= 3:\n",
        "                    word, _, tag = parts\n",
        "                    word_set.add(word)\n",
        "                    if tag not in tag2idx:\n",
        "                        tag2idx[tag] = len(tag2idx)\n",
        "    return word_set\n",
        "\n",
        "# Build vocab from all datasets\n",
        "vocab = set()\n",
        "for path in [train_path, val_path, test_path]:\n",
        "    vocab |= build_vocab_from_file(path, word2idx, tag2idx)\n",
        "\n",
        "# Step 3: Prepare embedding matrix\n",
        "embedding_dim = 300\n",
        "embedding_matrix = []\n",
        "embedding_matrix.append(np.zeros(embedding_dim))  # PAD\n",
        "embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))  # UNK\n",
        "\n",
        "for word in vocab:\n",
        "    word2idx[word] = len(word2idx)\n",
        "    if word in fasttext_model:\n",
        "        embedding_matrix.append(fasttext_model[word])\n",
        "    else:\n",
        "        embedding_matrix.append(np.random.uniform(-0.25, 0.25, embedding_dim))\n",
        "\n",
        "embedding_matrix = torch.tensor(embedding_matrix).float()\n",
        "\n",
        "# Step 4: Now it's safe to create datasets\n",
        "train_data = CustomDataset(train_path, word2idx, tag2idx)\n",
        "val_data = CustomDataset(val_path, word2idx, tag2idx)\n",
        "test_data = CustomDataset(test_path, word2idx, tag2idx)\n",
        "\n",
        "\n",
        "train_load = DataLoader(train_data, batch_size=32, collate_fn=collate_fn)\n",
        "val_load = DataLoader(val_data, batch_size=32, collate_fn=collate_fn)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unique_tags = extract_unique_tags(train_path)\n",
        "# tag2idx = {tag: idx for idx, tag in enumerate(unique_tags)}\n",
        "# tag2idx[\"<PAD>\"] = len(tag2idx)  # Add PAD if needed\n",
        "# tag2idx[\"<UNK>\"] = len(tag2idx)  # Optional: Add UNK tag\n",
        "\n",
        "# print(\"Number of tags:\", len(tag2idx))\n",
        "# print(tag2idx)\n"
      ],
      "metadata": {
        "id": "fnTYo5VJT5mf"
      },
      "id": "fnTYo5VJT5mf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Flatten the list of lists into a single list of tags\n",
        "all_tags = [tag for seq in train_data.ner_tags for tag in seq]\n",
        "\n",
        "# Count the frequency of each tag\n",
        "tag_counts = Counter(all_tags)\n",
        "\n",
        "# Display the counts\n",
        "for tag, count in tag_counts.items():\n",
        "    print(f\"{tag}: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMbRy8Jnlfmq",
        "outputId": "858b80d5-f9e2-45be-b898-4bead1675d2b"
      },
      "id": "MMbRy8Jnlfmq",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "B-LOC: 9395\n",
            "I-LOC: 4015\n",
            "E-LOC: 9395\n",
            "O: 167547\n",
            "S-NUM: 3882\n",
            "B-DATE: 599\n",
            "I-DATE: 388\n",
            "E-DATE: 599\n",
            "S-PER: 1911\n",
            "S-LOC: 991\n",
            "S-DATE: 699\n",
            "B-ORG: 308\n",
            "E-ORG: 308\n",
            "S-ORG: 184\n",
            "I-ORG: 208\n",
            "B-PER: 281\n",
            "E-PER: 281\n",
            "B-TIME: 143\n",
            "E-TIME: 143\n",
            "B-NUM: 151\n",
            "E-NUM: 151\n",
            "S-TIME: 118\n",
            "I-TIME: 92\n",
            "I-NUM: 32\n",
            "I-PER: 16\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_load:\n",
        "    sentences, tags = batch\n",
        "    print(\"Batch max len:\", sentences.shape)\n",
        "    print(\"tags:\", tags.shape)\n",
        "    break\n"
      ],
      "metadata": {
        "id": "OWOoIDb5IIfc",
        "outputId": "6fd3ad24-36a7-4aff-a534-337b0c168ead",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "OWOoIDb5IIfc",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch max len: torch.Size([32, 41])\n",
            "tags: torch.Size([32, 41])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,max_len,d_model):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len,d_model)\n",
        "        position = torch.arange(max_len).unsqueeze(1) # shape[max_len,1]\n",
        "        div_term = torch.exp((torch.arange(0, d_model, 2, dtype=torch.float) *-(math.log(10000.0) / d_model)))\n",
        "        pe[:, 0::2] = torch.sin(position.float() * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position.float() * div_term)\n",
        "        self.register_buffer('pe', pe)  # Register the positional encoding tensor as a buffer\n",
        "\n",
        "    def forward(self,x):\n",
        "        # x (batch_size,max_len, d_model)\n",
        "        x = x + self.pe[:x.size(1)] #BroadCast positional encoding\n",
        "        return x"
      ],
      "metadata": {
        "id": "QEbeIbS5tua8"
      },
      "id": "QEbeIbS5tua8",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Feedforward neural network with dropout and GELU activation.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Factor by which the hidden layer dimension will expand\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout):\n",
        "        super(FeedForward, self).__init__()\n",
        "        hidden_dim = dim * expension_factor\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.dropout1(F.gelu(self.fc1(x)))  # Apply GELU and dropout on the first layer\n",
        "        return self.dropout2(self.fc2(x))  # Apply second linear layer with dropout\n",
        "\n",
        "class Fourier(nn.Module):\n",
        "    \"\"\"\n",
        "    Fourier layer applying FFT twice (along two dimensions).\n",
        "\n",
        "    Parameters:\n",
        "    - dropout: Dropout probability\n",
        "    \"\"\"\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(Fourier, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply FFT along the last dimension (real and imaginary parts)\n",
        "        x = x.to(dtype=torch.float64)\n",
        "        x = torch.fft.fft(x, dim=-1)\n",
        "        x = torch.fft.fft(x, dim=1)  # Apply FFT along the second dimension as well\n",
        "        x = self.act(x.real)  # Using only real part\n",
        "        x = x.to(dtype=torch.float32)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class FNet(nn.Module):\n",
        "    \"\"\"\n",
        "    FNet model combining Fourier transformations and FeedForward layers.\n",
        "\n",
        "    Parameters:\n",
        "    - dim: Input dimension\n",
        "    - expension_factor: Expansion factor for FeedForward layers\n",
        "    - dropout: Dropout probability for regularization\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, expension_factor, dropout,embedding_matrix):\n",
        "\n",
        "        super(FNet, self).__init__()\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=False)  # Freeze embeddings\n",
        "        self.pos_en = PositionalEncoding(300, d_model = dim)\n",
        "        self.fourier = Fourier(dropout)\n",
        "        self.ffn = FeedForward(dim, expension_factor, dropout)\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.pos_en(x)\n",
        "        residual = x\n",
        "        x = self.fourier(x)\n",
        "        x = self.norm1(x + residual)\n",
        "        residual = x\n",
        "        x = self.ffn(x)\n",
        "        out = self.norm2(x + residual)\n",
        "        return out\n",
        "\n",
        "# Final NER model with classification head\n",
        "class FNetNER(nn.Module):\n",
        "    def __init__(self, embedding_matrix, num_tags, expension_factor=4, dropout=0.3):\n",
        "        super(FNetNER, self).__init__()\n",
        "        dim = embedding_matrix.shape[1]\n",
        "        self.fnet = FNet(dim, expension_factor, dropout, embedding_matrix)\n",
        "        self.classifier = nn.Linear(dim, num_tags)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fnet(x)  # [batch_size, seq_len, dim]\n",
        "        logits = self.classifier(x)  # [batch_size, seq_len, num_tags]\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "_f2UlPQHP4Hp"
      },
      "id": "_f2UlPQHP4Hp",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(embedding_matrix.shape)\n",
        "print(len(word2idx))\n",
        "print((len(tag2idx)))"
      ],
      "metadata": {
        "id": "688viprCUUux",
        "outputId": "9fdc2eca-90cc-4c45-a23d-3ec92e8ab892",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "688viprCUUux",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([19304, 300])\n",
            "19304\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = FNetNER(embedding_matrix, num_tags=len(tag2idx), expension_factor=4, dropout=0.3).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr =3e-4)\n"
      ],
      "metadata": {
        "id": "Nbpa5kOXSs3q"
      },
      "id": "Nbpa5kOXSs3q",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "labels = []\n",
        "for tag_seq in train_data.ner_tags:\n",
        "    labels.extend(tag_seq)\n",
        "\n",
        "\n",
        "label_ids = [tag2idx[tag] for tag in labels]\n",
        "present_classes = np.unique(label_ids)\n",
        "present_weights = compute_class_weight(class_weight='balanced', classes=present_classes, y=label_ids)\n",
        "\n",
        "full_weights = np.ones(len(tag2idx))\n",
        "for i, cls in enumerate(present_classes):\n",
        "    full_weights[cls] = present_weights[i]\n",
        "\n",
        "weights = torch.tensor(full_weights, dtype=torch.float).to(device)\n",
        "\n",
        "# Define loss\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[\"<PAD>\"], weight=weights)\n"
      ],
      "metadata": {
        "id": "DJN2lrE_uKAV"
      },
      "id": "DJN2lrE_uKAV",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    model.train()\n",
        "    for sentences, tags in train_load:\n",
        "        sentences, tags = sentences.to(device), tags.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(sentences)\n",
        "\n",
        "        outputs = outputs.view(-1, outputs.shape[-1])  # [batch_size * seq_len, num_tags]\n",
        "        tags = tags.view(-1)                           # [batch_size * seq_len]\n",
        "\n",
        "        loss = criterion(outputs, tags)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    avg_train_loss = total_loss / len(train_load)\n",
        "\n",
        "    val_loss = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for sentences, tags in val_load:\n",
        "            sentences, tags = sentences.to(device), tags.to(device)\n",
        "            outputs = model(sentences)\n",
        "\n",
        "            outputs = outputs.view(-1, outputs.shape[-1])\n",
        "            tags = tags.view(-1)\n",
        "            loss = criterion(outputs, tags)\n",
        "            val_loss += loss.item()\n",
        "    avg_val_loss = val_loss / len(val_load)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f} - Val Loss: {avg_val_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ZacBXeYsUyo7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "2a3ecf34-37d1-49ed-f7b7-a3cc9c0a7aec"
      },
      "id": "ZacBXeYsUyo7",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Train Loss: 1.8073 - Val Loss: 4.1352\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-9221dda43436>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test case\n",
        "test_sentence = [\"ရန်\",\"ကုန်\", \"မြို့\", \"သို့\", \"သွား\",\"သည်။\"]\n",
        "word_ids = [word2idx.get(word, word2idx[\"<UNK>\"]) for word in test_sentence]\n",
        "input_tensor = torch.tensor([word_ids], dtype=torch.long).to(device)  # Shape: [1, seq_len]\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    output = model(input_tensor)  # [1, seq_len, num_tags]\n",
        "    predicted_ids = torch.argmax(output, dim=-1).squeeze(0).tolist()  # Remove batch dim\n",
        "# Invert the tag2idx dictionary\n",
        "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
        "predicted_tags = [idx2tag[idx] for idx in predicted_ids]\n",
        "for word, tag in zip(test_sentence, predicted_tags):\n",
        "    print(f\"{word}\\t{tag}\")\n"
      ],
      "metadata": {
        "id": "2iCktKo3WXx6",
        "outputId": "0334f048-84db-4419-ff31-b72340cfb5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "2iCktKo3WXx6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ရန်\tO\n",
            "ကုန်\tO\n",
            "မြို့\tO\n",
            "သို့\tO\n",
            "သွား\tO\n",
            "သည်။\tO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save only the model's parameters\n",
        "torch.save(model.state_dict(), \"ner_model.pth\")"
      ],
      "metadata": {
        "id": "zekIuzU_BSry"
      },
      "id": "zekIuzU_BSry",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "END"
      ],
      "metadata": {
        "id": "WtsFBuwgcU9r"
      },
      "id": "WtsFBuwgcU9r"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}